pub mod api;
pub mod app;
pub mod archive;
pub mod handler;
pub mod markdown;
pub mod model;
pub mod render;
pub mod skill;
pub mod theme;
pub mod tools;
pub mod ui;

use crate::command::chat::theme::ThemeName;
use crate::config::YamlConfig;
use crate::{error, info};
use api::call_openai_stream;
use handler::run_chat_tui;
use model::{
    AgentConfig, ChatMessage, ModelProvider, agent_config_path, load_agent_config,
    load_system_prompt, save_agent_config, save_system_prompt,
};
use std::io::{self, Write};

pub fn handle_chat(content: &[String], _config: &YamlConfig) {
    let mut agent_config = load_agent_config();
    if let Some(file_prompt) = load_system_prompt() {
        agent_config.system_prompt = Some(file_prompt);
    } else if let Some(config_prompt) = agent_config.system_prompt.clone() {
        let _ = save_system_prompt(&config_prompt);
    }

    if agent_config.providers.is_empty() {
        info!("‚ö†Ô∏è  Â∞öÊú™ÈÖçÁΩÆ LLM Ê®°ÂûãÊèê‰æõÊñπ„ÄÇ");
        info!("üìÅ ËØ∑ÁºñËæëÈÖçÁΩÆÊñá‰ª∂: {}", agent_config_path().display());
        info!("üìù ÈÖçÁΩÆÁ§∫‰æã:");
        let example = AgentConfig {
            providers: vec![ModelProvider {
                name: "GPT-4o".to_string(),
                api_base: "https://api.openai.com/v1".to_string(),
                api_key: "sk-your-api-key".to_string(),
                model: "gpt-4o".to_string(),
            }],
            active_index: 0,
            system_prompt: None,
            stream_mode: true,
            max_history_messages: 20,
            theme: ThemeName::default(),
            tools_enabled: false,
            max_tool_rounds: 10,
        };
        if let Ok(json) = serde_json::to_string_pretty(&example) {
            println!("{}", json);
        }
        let _ = save_system_prompt("‰Ω†ÊòØ‰∏Ä‰∏™ÊúâÁî®ÁöÑÂä©Êâã„ÄÇ");
        // Ëá™Âä®ÂàõÂª∫Á§∫‰æãÈÖçÁΩÆÊñá‰ª∂
        if !agent_config_path().exists() {
            let _ = save_agent_config(&example);
            info!(
                "‚úÖ Â∑≤Ëá™Âä®ÂàõÂª∫Á§∫‰æãÈÖçÁΩÆÊñá‰ª∂: {}",
                agent_config_path().display()
            );
            info!("üìå ËØ∑‰øÆÊîπÂÖ∂‰∏≠ÁöÑ api_key ÂíåÂÖ∂‰ªñÈÖçÁΩÆÂêéÈáçÊñ∞ËøêË°å chat ÂëΩ‰ª§");
        }
        return;
    }

    if content.is_empty() {
        // Êó†ÂèÇÊï∞ÔºöËøõÂÖ• TUI ÂØπËØùÁïåÈù¢
        run_chat_tui();
        return;
    }

    // ÊúâÂèÇÊï∞ÔºöÂø´ÈÄüÂèëÈÄÅÊ∂àÊÅØÂπ∂ÊâìÂç∞ÂõûÂ§ç
    let message = content.join(" ");
    let message = message.trim().to_string();
    if message.is_empty() {
        error!("‚ö†Ô∏è Ê∂àÊÅØÂÜÖÂÆπ‰∏∫Á©∫");
        return;
    }

    let idx = agent_config
        .active_index
        .min(agent_config.providers.len() - 1);
    let provider = &agent_config.providers[idx];

    info!("ü§ñ [{}] ÊÄùËÄÉ‰∏≠...", provider.name);

    let mut messages = Vec::new();
    messages.push(ChatMessage::text("user", message));

    match call_openai_stream(
        provider,
        &messages,
        agent_config.system_prompt.as_deref(),
        &mut |chunk| {
            print!("{}", chunk);
            let _ = io::stdout().flush();
        },
    ) {
        Ok(_) => {
            println!(); // Êç¢Ë°å
        }
        Err(e) => {
            error!("\n‚ùå {}", e);
        }
    }
}
